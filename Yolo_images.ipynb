{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMa3Du7niT5qx4mhhS/Ll2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drpetros11111/CompVisionYOLO/blob/main/Yolo_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkaZv3KTAFEQ"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolo11n.pt')  # 'n' denotes the nano version; choose according to your needs\n",
        "\n",
        "model.train(data='D:\\\\lioness-detection.v1i.yolov11\\\\data.yaml', epochs=50, save=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This code snippet is used for object detection using the YOLOv11 model\n",
        "\n",
        "--------------\n",
        "    import cv2\n",
        "\n",
        "Imports the OpenCV library, which is commonly used for computer vision tasks.\n",
        "\n",
        "----------------\n",
        "\n",
        "    from ultralytics import YOLO\n",
        "\n",
        "Imports the YOLO class from the ultralytics library, which provides tools for working with YOLO models.\n",
        "\n",
        "------------\n",
        "\n",
        "    model = YOLO('yolo11n.pt')\n",
        "\n",
        "Loads a pre-trained YOLOv11 nano model. The .pt file contains the model's weights.\n",
        "\n",
        "------------------------\n",
        "    model.train(data='D:\\\\lioness-detection.v1i.yolov11\\\\data.yaml', epochs=50, save=False)\n",
        "\n",
        "This line starts the training process for the loaded YOLO model.\n",
        "\n",
        "-------------------\n",
        "    data='D:\\\\lioness-detection.v1i.yolov11\\\\data.yaml'\n",
        "\n",
        "Specifies the path to the YAML file that contains the configuration for your training data (dataset location, class names, etc.).\n",
        "\n",
        "---------------------\n",
        "    epochs=50\n",
        "\n",
        "Sets the number of training epochs to 50. An epoch is one complete pass through the entire training dataset.\n",
        "\n",
        "-------------------\n",
        "    save=False\n",
        "\n",
        "Prevents saving the model checkpoints during training.\n",
        "\n",
        "--------------------\n",
        "#In summary,\n",
        "this code sets up a YOLOv11 model and trains it on a custom dataset for 50 epochs without saving intermediate checkpoints.\n",
        "\n"
      ],
      "metadata": {
        "id": "7-9PYRJAqlW9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lG31TU3AqKPQ"
      },
      "outputs": [],
      "source": [
        "# Open the input video\n",
        "video_path = 'lion_video2.mp4'  # Replace with your video file path\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# get the original video properties\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Define the codec and create a VideoWriter object\n",
        "output_path = \"output_lion1.mp4\"\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4 format\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This code snippet handles the input and output video streams for processing\n",
        "\n",
        "-----------------------------\n",
        "\n",
        "    video_path = 'lion_video2.mp4'\n",
        "\n",
        "Sets the path to the input video file. You should replace 'lion_video2.mp4' with the actual path to your video file.\n",
        "\n",
        "------------------------------\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "Creates a VideoCapture object, which is used to read video frames from the specified file.\n",
        "\n",
        "-------------------------\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "\n",
        "Gets the width of the frames in the input video.\n",
        "\n",
        "-----------------------------------\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "Gets the height of the frames in the input video.\n",
        "\n",
        "------\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "Gets the frames per second (fps) of the input video.\n",
        "\n",
        "----------------\n",
        "    output_path = \"output_lion1.mp4\"\n",
        "\n",
        "Sets the path and filename for the output video where the processed frames will be saved.\n",
        "\n",
        "------------\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "\n",
        "Defines the video codec to be used for the output video. 'mp4v' is a common codec for creating .mp4 files.\n",
        "\n",
        "------------------------------\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "Creates a VideoWriter object, which is used to write processed frames to the specified output file with the defined codec, fps, and frame size.\n",
        "\n",
        "----------------------\n",
        "#In essence,\n",
        "this code opens the input video, gets its properties (width, height, fps), and sets up an object to write the processed frames to a new output video file with the same properties."
      ],
      "metadata": {
        "id": "STT6QEBEwKBl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBcd6AylqOz8"
      },
      "outputs": [],
      "source": [
        "# Process each frame\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Perform object detection\n",
        "    results = model(frame)\n",
        "\n",
        "    # Annotate the frame with detection results\n",
        "    annotated_frame = results[0].plot()\n",
        "\n",
        "    # Display the frame (press 'q' to quit)\n",
        "    cv2.imshow('YOLOv11 Detection', annotated_frame)\n",
        "    # Write the annotated frame to the output video\n",
        "    out.write(annotated_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This code processes each frame of the video for object detection\n",
        "\n",
        "-----------------------\n",
        "\n",
        "    while cap.isOpened():\n",
        "\n",
        "This loop continues as long as the video file (cap) is open.\n",
        "\n",
        "----------------------------------------\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "Reads the next frame from the video.\n",
        "\n",
        "ret is a boolean indicating if the frame was successfully read, and frame is the image data of the frame.\n",
        "\n",
        "-------------------------------------\n",
        "    if not ret:\n",
        "\n",
        "If ret is False, it means the video has ended or there was an error reading the frame, so the loop breaks.\n",
        "\n",
        "-------------\n",
        "    results = model(frame)\n",
        "This is where the object detection happens.\n",
        "\n",
        "The model (which is the trained YOLO model) is applied to the current frame to get the detection results.\n",
        "\n",
        "------------------------------\n",
        "    annotated_frame = results[0].plot()\n",
        "\n",
        "This line takes the first result from the detection (results[0]) and draws bounding boxes, labels, and confidence scores onto the original frame, creating an annotated_frame.\n",
        "\n",
        "---------------------\n",
        "    cv2.imshow('YOLOv11 Detection', annotated_frame)\n",
        "\n",
        "This displays the annotated_frame in a window titled 'YOLOv11 Detection'.\n",
        "\n",
        "You would typically need to run this in an environment that supports GUI windows, which might not be the case in a default Colab environment.\n",
        "\n",
        "---------------------\n",
        "    out.write(annotated_frame)\n",
        "This writes the annotated_frame to the output video file that was set up earlier using cv2.VideoWriter.\n",
        "\n",
        "------------------------\n",
        "#In summary,\n",
        "this code iterates through the video frames, performs object detection on each frame using the YOLO model, annotates the frames with the detection results, displays the annotated frames (if the environment supports it), and writes the annotated frames to a new video file."
      ],
      "metadata": {
        "id": "3ls4eKrX0YzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PtQiJ1t_0cEq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17IuawNHz70O"
      },
      "outputs": [],
      "source": [
        "# Release resources\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ]
}